{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The original-ish jupyter notebook\n",
    "This notebook was the first real \"proof of concept\"/working search engine Megan and I made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\halom\\anaconda3\\envs\\test2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "from collections import Counter\n",
    "import string\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"code_search_net\", \"all\")\n",
    "\n",
    "dataset_dict = datasets.load_from_disk(\"./Dataset/CodeSearchCorpus/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if the pytorch GPU functions work\n",
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.is_available()) #We have GPU on deck and ready\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the size of the CodeSearchNet database\n",
    "print(len(dataset_dict[\"train\"]))\n",
    "print(len(dataset_dict[\"validation\"]))\n",
    "print(len(dataset_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only the training dataset\n",
    "train_dataset = dataset_dict[\"train\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing the test_dataset\n",
    "# test_dataset = dataset_dict[\"test\"]\n",
    "# test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, 1.8M is too much. For week 5 at least, we've decided to train on a random sample of 10k from the training, 1k validation and 1k test\n",
    "\n",
    "Column for semantic search: func_documentation_string\n",
    "Column for tfidf: func_code_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing what one sample row of the training dataset is like\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inverted_index_50000.pkl train_subset_embeddings_dataset_50000.pkl\n"
     ]
    }
   ],
   "source": [
    "# Decide number of rows, the filepath to where to store the pickle files\n",
    "# The pickled objects are are the inverted index and embeddings dataset\n",
    "\n",
    "num_rows = 50000\n",
    "filepath_pkl_obj = \"./PickleObjects/\"\n",
    "inverted_index_name = f\"inverted_index_{num_rows}.pkl\"\n",
    "tsed_name = f\"train_subset_embeddings_dataset_{num_rows}.pkl\"\n",
    "\n",
    "print(inverted_index_name, tsed_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a sample of the training dataset\n",
    "# There are SO MANY PROBLEMS WHEN WE DO THIS THO, need to ask colin what to do i suppose?\n",
    "\n",
    "np.random.seed(1)\n",
    "train_subset_indices = np.random.choice(len(train_dataset), num_rows, replace = False)\n",
    "train_dataset_subset = train_dataset.select(train_subset_indices)\n",
    "\n",
    "len(train_dataset_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Embeddings Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following code from: https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt\n",
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" #Can/Should test different models\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model to the GPU. Mine is a 3060\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Hugging Face Tutorials\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train embeddings\n",
    "#If the filename exists, load the pickle object. If not, train it and then save it as a pickle object\n",
    "#REMEMBER TO KEEP THE FILENAMES THE SAME 0_0\n",
    "try:\n",
    "    with open(f'{filepath_pkl_obj}{tsed_name}', 'rb') as f:  # open a text file\n",
    "        train_subset_embeddings_dataset = pickle.load(f) # serialize the list\n",
    "        f.close()\n",
    "except:\n",
    "    train_subset_embeddings_dataset = train_dataset_subset.map(\n",
    "        lambda x: {\"embeddings\": get_embeddings(x[\"func_documentation_string\"]).detach().cpu().numpy()[0]}\n",
    "    )\n",
    "\n",
    "    train_subset_embeddings_dataset.add_faiss_index(column=\"embeddings\")\n",
    "\n",
    "    with open(f'{filepath_pkl_obj}{tsed_name}', 'wb') as f:  # open a text file\n",
    "        pickle.dump(train_subset_embeddings_dataset, f) # serialize the list\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_embeddings_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the embeddings to a pandas dataframe\n",
    "tsed_DF = train_subset_embeddings_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean the code tokens. Super rudimentary, \n",
    "# as of right now, we're just taking rid of the single punctuation\n",
    "def clean_code_tokens(lst):\n",
    "    result = string.punctuation \n",
    "    new_lst = [] \n",
    "    for character in lst:\n",
    "        if character in result:\n",
    "            continue\n",
    "        else:\n",
    "            new_lst.append(character)\n",
    "    return new_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a column of \"clean\" code tokens\n",
    "# There's many many issues with this strategy\n",
    "tsed_DF[\"clean_code_tokens\"] =  tsed_DF[\"func_code_tokens\"].apply(clean_code_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Much of this code was based off of William Scott's implementation of TF-IDF: https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates s list of documents\n",
    "documents = tsed_DF[\"clean_code_tokens\"].to_dict()\n",
    "\n",
    "# Compiles a list of the words \n",
    "all_words = []\n",
    "for i in list(tsed_DF[\"clean_code_tokens\"].to_dict().values()):\n",
    "    all_words += i\n",
    "\n",
    "#convert all words to a set, eliminates, duplicates\n",
    "all_words = list(set(all_words)) #Get rid of all repeats\n",
    "# all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{filepath_pkl_obj}{inverted_index_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index tf_idf\n",
    "# tf_idf = create_tfidf(num_rows, tsed_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into much more efficient method of querying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tf_idf_query(query_string, inverted_index, tf_idf, k = 10):\n",
    "#     query_tokens = query_string.split()\n",
    "\n",
    "#     rel_indices = []\n",
    "    \n",
    "#     for token in query_tokens:\n",
    "#         if token in inverted_index:\n",
    "#             rel_indices += list(inverted_index[token].keys())\n",
    "    \n",
    "#     rel_indices = set(rel_indices)\n",
    "\n",
    "#     result_lst = []\n",
    "#     for i in rel_indices:\n",
    "#         for token in query_tokens:\n",
    "#             score = 0\n",
    "#             try:\n",
    "#                 score += (tf_idf[(i, token)])\n",
    "#             except: continue\n",
    "#         result_lst.append([i, score])\n",
    "    \n",
    "#     result_lst.sort(reverse=True, key = lambda x: x[1])\n",
    "#     return result_lst[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inverted index if its not in a pickle file (and save it)\n",
    "def create_inverted_index(filepath_to_search):\n",
    "    try:\n",
    "        with open(filepath_to_search, 'rb') as f:\n",
    "            inverted_index = pickle.load(f) # deserialize using load()\n",
    "            f.close()\n",
    "    except:\n",
    "        inverted_index = {}\n",
    "        for i in range(num_rows):\n",
    "            token_counter = Counter(tsed_DF.iloc[i][\"clean_code_tokens\"])\n",
    "\n",
    "            for token in token_counter:\n",
    "                if token not in inverted_index:\n",
    "                    inverted_index[token] = {}\n",
    "                inverted_index[token][i] = token_counter[token]\n",
    "        \n",
    "        #Pickle afterwards\n",
    "        with open(filepath_to_search, 'wb') as f:  # open a text file\n",
    "            pickle.dump(inverted_index, f) # serialize the list\n",
    "            f.close()\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = create_inverted_index(f'{filepath_pkl_obj}{inverted_index_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get the document frequency of a word/token\n",
    "# def doc_freq(word):\n",
    "#     c = 0\n",
    "#     try:\n",
    "#         c = inverted_index[word]\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     if type(c) == list:\n",
    "#         return len(c)\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tf_idf object. WILL TURN THIS INTO A FUNCTION LATER\n",
    "def create_tfidf(num_rows, tsed_DF):\n",
    "    tf_idf = {}\n",
    "    for i in range(num_rows):\n",
    "        # print(i)\n",
    "        tokens = tsed_DF[\"clean_code_tokens\"].iloc[i]\n",
    "        counter = Counter(tokens)\n",
    "        words_count = len(tokens)\n",
    "\n",
    "        for token in np.unique(tokens):\n",
    "            tf = counter[token] / words_count\n",
    "            df = len(inverted_index[token])\n",
    "            idf = np.log((num_rows + 1) / (df + 1))\n",
    "\n",
    "            tf_idf[i, token] = tf * idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted_index tf_idf\n",
    "tf_idf = create_tfidf(num_rows, tsed_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into much more efficient method of querying results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cosine_similarity. #Look into np.cos Annoy FAISS. look into applying and vectorizing\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_results(query_string, inverted_index, tf_idf, k = 10):\n",
    "    query_tokens = query_string.split()\n",
    "\n",
    "    rel_indices = []\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            rel_indices += list(inverted_index[token].keys())\n",
    "    \n",
    "    rel_indices = set(rel_indices)\n",
    "\n",
    "    query_embedding = get_embeddings([query_string]).cpu().detach().numpy()\n",
    "    # len(query_embedding[0])\n",
    "    # len(tsed_DF[\"embeddings\"][0])\n",
    "    \n",
    "\n",
    "    result_lst = []\n",
    "    for i in rel_indices:\n",
    "        for token in query_tokens:\n",
    "            tf_score = 0\n",
    "            try:\n",
    "                tf_score += (tf_idf[(i, token)])\n",
    "            except: continue #this is bad, make sure this isn't the play\n",
    "        # print(i)\n",
    "\n",
    "        result_lst.append([i, tf_score, cosine_sim(tsed_DF[\"embeddings\"][i], query_embedding[0])])\n",
    "    \n",
    "    result_lst.sort(reverse=True, key = lambda x: 0.5 * x[1] + 0.5*x[2])\n",
    "    return result_lst[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different approaches to cosine comparisons\n",
    "\n",
    "Conclusion: Calculating the cosine similarities by brute force for all of them is extremely computationally intensive. Pivot to only looking into docs that have the tokens. Can reduce extremely. And calculate the tfidf like that as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vector = gen_vector(\"pandas how to select first 10 rows\")\n",
    "\n",
    "# for i, x in enumerate(tf_idf_array):\n",
    "#     (cosine_sim(x, test_vector))\n",
    "\n",
    "# for i, x in enumerate(tf_idf_array):\n",
    "#     (1 - spatial.distance.cosine(x, test_vector))\n",
    "\n",
    "# tfidf_series = pd.Series(list(tf_idf_array))\n",
    "\n",
    "# vectorized_test = tfidf_series.apply(lambda x: cosine_sim(x, test_vector))\n",
    "\n",
    "# tfidf_series.apply(lambda x: np.dot(x, test_vector)/(np.linalg.norm(x)*np.linalg.norm(test_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_matches(\"read csv to dataframe\", 10, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_results = query_results(\"string to date\", inverted_index, tf_idf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tqr in test_query_results:\n",
    "#     display(tsed_DF.iloc[tqr[0]][[\"func_name\", \"language\",  \"func_code_string\", \"func_documentation_string\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function which runs all 99 queries, and returns a pd df of the results\n",
    "def create_results(query_filepath, results_per_query = 100):\n",
    "    queries = pd.read_csv(query_filepath)\n",
    "    # display(queries)\n",
    "    q_lst = queries[\"query\"].to_list()\n",
    "    # print(q_lst)\n",
    "\n",
    "    lang_lst = []\n",
    "    func_code_url_lst = []\n",
    "    query_lst = []\n",
    "\n",
    "    for i, query in enumerate(q_lst):\n",
    "        # print(i)\n",
    "        fbm_lst = query_results(query, inverted_index, tf_idf, results_per_query)\n",
    "        query_lst += [query for j in range(len(fbm_lst))]\n",
    "        \n",
    "        for lst in fbm_lst:\n",
    "            # print(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "            # print(tsed_DF.iloc[lst[0]][\"func_name\"])\n",
    "            # print(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "            # print(f\"SCORE: {lst[1]}\")\n",
    "            # print(\"-\" * 100)\n",
    "\n",
    "            lang_lst.append(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "            func_code_url_lst.append(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "        \n",
    "        # break\n",
    "\n",
    "    # print(lang_lst)\n",
    "    # print(func_code_url_lst)\n",
    "    # print(query_lst)\n",
    "    prediction_df = pd.DataFrame({'language' : lang_lst, 'url': func_code_url_lst, \"query\" : query_lst})\n",
    "    return prediction_df\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = create_results(\"./Dataset/Testing/queries.csv\", results_per_query=50)\n",
    "res_df.to_csv(\"./csv_output/baseline_50k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in test_query_results:\n",
    "#     # print(len(tsed_DF.iloc[res[0]][\"embeddings\"]))\n",
    "#     print(\"-\" * 100)\n",
    "\n",
    "# query_embedding = get_embeddings([\"string to date\"]).cpu().detach().numpy()\n",
    "    \n",
    "    \n",
    "# desc_scores, desc_results = train_subset_embeddings_dataset.get_nearest_examples(\"embeddings\", query_embedding, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_scores\n",
    "# desc_results.keys()\n",
    "# test_df = pd.DataFrame(desc_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsed_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embedding = get_embeddings([\"string to date\"]).cpu().detach().numpy()\n",
    "# len(query_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_embedding = get_embeddings([\"string to date\"]).cpu().detach().numpy()\n",
    "# len(query_embedding[0])\n",
    "# len(tsed_DF[\"embeddings\"][0])\n",
    "# cosine_sim(tsed_DF[\"embeddings\"][0], query_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_query = \"string to date\"\n",
    "# test_query_tokens = test_query.split()\n",
    "# rel_indices = []\n",
    "# for token in test_query_tokens:\n",
    "#     # print(inverted_index[token])\n",
    "#     # print(token in inverted_index)\n",
    "    \n",
    "#     if token in inverted_index:\n",
    "#         print(len(list(inverted_index[token].keys())))\n",
    "#         rel_indices += list(inverted_index[token].keys())\n",
    "#     #     num_docs_with_term = (len(inverted_index[token]))\n",
    "#     #     for i in inverted_index[token].keys():\n",
    "#     #         print(tf_idf[(i, token)])\n",
    "\n",
    "# # len(rel_indices) == 2074 + 83 + 99\n",
    "# rel_indices = set(rel_indices)\n",
    "# len(rel_indices)\n",
    "\n",
    "# test_answers = []\n",
    "# for i in rel_indices:\n",
    "#     # print(i)\n",
    "#     for token in test_query_tokens:\n",
    "#         score = 0\n",
    "#         try:\n",
    "#             score += (tf_idf[(i, token)])\n",
    "#         except: continue\n",
    "#     test_answers.append([i, score])\n",
    "\n",
    "# test_answers.sort(key = lambda x: x[1], reverse=True)\n",
    "# test_answers[:10]\n",
    "# tsed_DF.iloc[19040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tf_idf\n",
    "# all_words_dict = dict(zip(all_words, range(len(all_words))))\n",
    "\n",
    "\n",
    "# tf_idf_array = np.zeros((num_rows, len(all_words)), dtype=\"float32\")\n",
    "\n",
    "# for i in tf_idf:\n",
    "#     try:\n",
    "#         ind = all_words_dict[i[1]]\n",
    "#         tf_idf_array[i[0]][ind] = tf_idf[i]\n",
    "#     except:\n",
    "#         pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get the document frequency of a word/token\n",
    "# def doc_freq(word):\n",
    "#     c = 0\n",
    "#     try:\n",
    "#         c = inverted_index[word]\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     if type(c) == list:\n",
    "#         return len(c)\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to get the document frequency of a word/token\n",
    "# def doc_freq(word):\n",
    "#     c = 0\n",
    "#     try:\n",
    "#         c = inverted_index[word]\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "#     if type(c) == list:\n",
    "#         return len(c)\n",
    "#     else:\n",
    "#         return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function which given a query, returns in a tf_idf vector\n",
    "# def gen_vector(s):\n",
    "#     # This is where we'd do more processing of the query\n",
    "#     tokens = s.split()\n",
    "\n",
    "#     q_vector = np.zeros((len(all_words)))\n",
    "    \n",
    "#     counter = Counter(tokens)\n",
    "#     words_count = len(tokens)\n",
    "\n",
    "#     for token in np.unique(tokens):\n",
    "        \n",
    "#         tf = counter[token]/words_count\n",
    "#         try:\n",
    "#             df = len(inverted_index[token])\n",
    "#         except:\n",
    "#             df = 0\n",
    "#         # df = doc_freq(token)\n",
    "#         idf = np.log((num_rows+1)/(df+1))\n",
    "\n",
    "#         try:\n",
    "#             ind = all_words_dict[token]\n",
    "#             q_vector[ind] = tf*idf\n",
    "#         except:\n",
    "#             pass\n",
    "#     return q_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_vector(\"pandas how to select first 10 rows\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_find_best_tfidf(query, df, colname):\n",
    "#     query_vector = gen_vector(query)\n",
    "#     df[\"cosine_sim\"] = df[colname].apply(lambda x: 1 - (spatial.distance.cosine(query_vector, x)))\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_find_best_tfidf(\"pandas how to select first 10 rows\", tsed_DF, \"tf_idf_vector\")\n",
    "# [ for x in range(num_rows)]\n",
    "# test_q_vector = gen_vector(\"pandas how to select first 10 rows\")\n",
    "# xa = tsed_DF[\"tf_idf_vector\"]\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code took 90s. That's buttcheeks\n",
    "# tsed_DF[\"tf_idf_vector\"].apply(lambda row: 1 - (spatial.distance.cosine(test_q_vector, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (np.linalg.norm(xa, axis = 1) * np.linalg.norm(test_q_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss.normalize_L2(xa)\n",
    "# vector_dimension = tf_idf_array.shape[1]\n",
    "# index = faiss.IndexFlatIP(vector_dimension)\n",
    "# faiss.normalize_L2(tf_idf_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.add(tf_idf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _vector = np.array([test_q_vector], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss.normalize_L2(test_q_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _vector.shape\n",
    "# _vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_find_best_tfidf(\"pandas how to select first 10 rows\", tsed_DF, \"tf_idf_vector\")\n",
    "# [ for x in range(num_rows)]\n",
    "# test_q_vector = gen_vector(\"pandas how to select first 10 rows\")\n",
    "# xa = tsed_DF[\"tf_idf_vector\"]\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code took 90s. That's buttcheeks\n",
    "# tsed_DF[\"tf_idf_vector\"].apply(lambda row: 1 - (spatial.distance.cosine(test_q_vector, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (np.linalg.norm(xa, axis = 1) * np.linalg.norm(test_q_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss.normalize_L2(xa)\n",
    "# vector_dimension = tf_idf_array.shape[1]\n",
    "# index = faiss.IndexFlatIP(vector_dimension)\n",
    "# faiss.normalize_L2(tf_idf_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.add(tf_idf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _vector = np.array([test_q_vector], dtype=\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss.normalize_L2(test_q_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _vector.shape\n",
    "# _vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for cosine_similarity. #Look into np.cos Annoy FAISS. look into applying and vectorizing\n",
    "def cosine_sim(a, b):\n",
    "    return np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to find the best match\n",
    "# query param: the string query\n",
    "# k param: the k number of results to return\n",
    "# alpha: the value which determines the linear split alpha * tfidf portion + (1-alpha)*semantic search portion\n",
    "# def find_best_matches(query, k, alpha = 0.5):\n",
    "#     q_vector = gen_vector(query)\n",
    "#     # q_embedding_vector = get_embeddings([query]).cpu().detach().numpy()[0]\n",
    "    \n",
    "    \n",
    "#     cosine_lst = []\n",
    "    \n",
    "#     for i, x in enumerate(tf_idf_array):\n",
    "#         # col = tfidf_DF[x].to_numpy()\n",
    "#         # Tensor.cpu()\n",
    "#         # embedding = tsed_DF.iloc[i][\"embeddings\"]\n",
    "\n",
    "#         # cosine_lst[i] = [i, (alpha) * cosine_sim(q_vector, x) + (1 - alpha) * cosine_sim(q_embedding_vector, embedding)]\n",
    "\n",
    "#         # cosine_lst[i] = [i, (alpha) * 1 - (spatial.distance.cosine(q_vector, x))]\n",
    "#         cosine_lst.append([i, (alpha) * cosine_sim(q_vector, x)])\n",
    "    \n",
    "    \n",
    "#     cosine_lst.sort(reverse = True, key = lambda x: x[1])\n",
    "#     return cosine_lst[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vector = gen_vector(\"pandas how to select first 10 rows\")\n",
    "\n",
    "# for i, x in enumerate(tf_idf_array):\n",
    "#     (cosine_sim(x, test_vector))\n",
    "\n",
    "# for i, x in enumerate(tf_idf_array):\n",
    "#     (1 - spatial.distance.cosine(x, test_vector))\n",
    "\n",
    "# tfidf_series = pd.Series(list(tf_idf_array))\n",
    "\n",
    "# vectorized_test = tfidf_series.apply(lambda x: cosine_sim(x, test_vector))\n",
    "\n",
    "# tfidf_series.apply(lambda x: np.dot(x, test_vector)/(np.linalg.norm(x)*np.linalg.norm(test_vector)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_best_matches(\"read csv to dataframe\", 10, alpha = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function which runs all 99 queries, and returns a pd df of the results\n",
    "# def create_results(query_filepath, results_per_query = 100):\n",
    "#     queries = pd.read_csv(query_filepath)\n",
    "#     # display(queries)\n",
    "#     q_lst = queries[\"query\"].to_list()\n",
    "#     # print(q_lst)\n",
    "\n",
    "#     lang_lst = []\n",
    "#     func_code_url_lst = []\n",
    "#     query_lst = []\n",
    "\n",
    "#     for i, query in enumerate(q_lst):\n",
    "#         print(i)\n",
    "#         fbm_lst = find_best_matches(query, results_per_query, 0.2)\n",
    "#         query_lst += [query for j in range(len(fbm_lst))]\n",
    "        \n",
    "#         for lst in fbm_lst:\n",
    "#             # print(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "#             # print(tsed_DF.iloc[lst[0]][\"func_name\"])\n",
    "#             # print(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "#             # print(f\"SCORE: {lst[1]}\")\n",
    "#             # print(\"-\" * 100)\n",
    "\n",
    "#             lang_lst.append(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "#             func_code_url_lst.append(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "        \n",
    "#         # break\n",
    "\n",
    "#     # print(lang_lst)\n",
    "#     # print(func_code_url_lst)\n",
    "#     # print(query_lst)\n",
    "#     prediction_df = pd.DataFrame({'language' : lang_lst, 'url': func_code_url_lst, \"query\" : query_lst})\n",
    "#     return prediction_df\n",
    "        \n",
    "# res_df = create_results(\"./Dataset/Testing/queries.csv\", results_per_query=50)\n",
    "# res_df.to_csv(\"./csv_output/baseline_20k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
