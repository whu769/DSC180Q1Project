{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal is to look into the large scale tokenization of the entire dataset\n",
    "\n",
    "\n",
    "#### Takeaways: We CAN TF-IDF 2 million, but memory is a big issue? That on top of not playing around with embeddings yet is a slightly sussy thought."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "from collections import Counter\n",
    "import string\n",
    "from scipy import spatial\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = datasets.load_from_disk(\"./Dataset/CodeSearchCorpus/\")\n",
    "train_dataset = dataset_dict[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.to_pandas() Not feasible, 90% memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_rows = 50000\n",
    "# np.random.seed(1)\n",
    "# train_subset_indices = np.random.choice(len(train_dataset), num_rows, replace = False)\n",
    "# train_dataset_subset = train_dataset.select(train_subset_indices)\n",
    "# len(train_dataset_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_subset.select_columns(\"func_documentation_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_subset = train_dataset_subset.select_columns(\"func_documentation_string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_subset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "st = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.select_columns(\"func_documentation_string\")\n",
    "\n",
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.map(lambda tkn: {\"func_doc_tokens\": tokenizer.tokenize(tkn)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_DF = train_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_DF[\"func_doc_tokens\"] = train_dataset_DF[\"func_documentation_string\"].apply(lambda x: tokenizer.tokenize(x))\n",
    "\n",
    "#25.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_DF[\"func_doc_stem_tokens\"] = train_dataset_DF[\"func_doc_tokens\"].apply(lambda x: [st.stem(word) for word in x])\n",
    "# ~35 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokens\n",
    "# [ps.stem(word) for word in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "for i in range(len(train_dataset_DF)):\n",
    "    token_counter = Counter(train_dataset_DF.iloc[i][\"func_doc_stem_tokens\"])\n",
    "\n",
    "    for token in token_counter:\n",
    "        if token not in inverted_index:\n",
    "            inverted_index[token] = {}\n",
    "        inverted_index[token][i] = token_counter[token]\n",
    "\n",
    "#Took ~2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#73622 No stemming\n",
    "#68995 with stemming\n",
    "#Pickle afterwards\n",
    "with open(\"./pickleObjects/inverted_index_FULL.pkl\", 'wb') as f:  # open a text file\n",
    "    pickle.dump(inverted_index, f) # serialize the list\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./pickleObjects/tfidf_FULL.pkl\", 'wb') as f:  # open a text file\n",
    "#     pickle.dump(tf_idf, f) # serialize the list\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickleObjects/inverted_index_FULL.pkl', 'rb') as f:\n",
    "    inverted_index = pickle.load(f) # deserialize using load()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {}\n",
    "for i in range(len(train_dataset_DF)):\n",
    "    # print(i)\n",
    "    tokens = train_dataset_DF[\"func_doc_stem_tokens\"].iloc[i]\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token] / words_count\n",
    "        df = len(inverted_index[token])\n",
    "        idf = np.log((len(train_dataset_DF) + 1) / (df + 1))\n",
    "\n",
    "        tf_idf[i, token] = tf * idf\n",
    "\n",
    "# 4 minutes there or thereabouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"./pickleObjects/tfidf_FULL.pkl\", 'wb') as f:  # open a text file\n",
    "#     pickle.dump(tf_idf, f) # serialize the list\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./pickleObjects/tfidf_FULL.pkl', 'rb') as f:\n",
    "#     tf_idf = pickle.load(f) # deserialize using load()\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_results(query_string, inverted_index, tf_idf, k = 10):\n",
    "    query_tokens = [st.stem(word) for word in tokenizer.tokenize(query_string)]\n",
    "    print(query_tokens)\n",
    "    rel_indices = []\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            rel_indices += list(inverted_index[token].keys())\n",
    "    \n",
    "    rel_indices = set(rel_indices)\n",
    "\n",
    "    # len(query_embedding[0])\n",
    "    # len(tsed_DF[\"embeddings\"][0])\n",
    "    \n",
    "\n",
    "    result_lst = []\n",
    "    for i in rel_indices:\n",
    "        for token in query_tokens:\n",
    "            tf_score = 0\n",
    "            try:\n",
    "                # print(tf_idf[(i, token)])\n",
    "                tf_score += (tf_idf[(i, token)])\n",
    "            except: \n",
    "                continue #this is bad, make sure this isn't the play\n",
    "        # print(i)\n",
    "\n",
    "        result_lst.append([i, tf_score])\n",
    "    \n",
    "    result_lst.sort(reverse=True, key = lambda x: x[1])\n",
    "    return result_lst[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_results(\"string to datetime\", inverted_index, tf_idf, k = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[100930]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pickleObjects/train_subset_embeddings_dataset_100000_cb.pkl\", 'rb') as f:  # open a text file\n",
    "            embed_dataset = pickle.load(f) # serialize the list\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
