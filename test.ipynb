{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\halom\\anaconda3\\envs\\test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# import string library function  \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"code_search_net\", \"all\")\n",
    "\n",
    "dataset_dict = datasets.load_from_disk(\"./Dataset/CodeSearchCorpus/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "CUDA device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.is_available()) #We have GPU on deck and ready\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1880853\n",
      "89154\n",
      "100529\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_dict[\"train\"]))\n",
    "print(len(dataset_dict[\"validation\"]))\n",
    "print(len(dataset_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 1880853\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = dataset_dict[\"train\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
       "    num_rows: 100529\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset_dict[\"test\"]\n",
    "test_dataset\n",
    "\n",
    "# Yeah, 1.8M is too much. For week 5 at least, we've decided to train on a random sample of 10k from the training, 1k validation and 1k test\n",
    "\n",
    "# Column for semantic search: func_documentation_string\n",
    "# Column for tfidf: func_code_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "train_subset_indices = np.random.choice(len(train_dataset), 1000, replace = False)\n",
    "train_dataset_subset = train_dataset.select(train_subset_indices)\n",
    "\n",
    "len(train_dataset_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device\n",
    "\n",
    "#Following code from: https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Hugging Face Tutorials\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://huggingface.co/docs/datasets/use_with_pytorch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained embeddings for semantic search portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:28<00:00, 35.68 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_subset_embeddings_dataset = train_dataset_subset.map(\n",
    "    lambda x: {\"embeddings\": get_embeddings(x[\"func_documentation_string\"]).detach().cpu().numpy()[0]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickleObjects/train_subset_embeddings_dataset_1000.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(train_subset_embeddings_dataset, f) # serialize the list\n",
    "    f.close()\n",
    "\n",
    "# with open('./pickleObjects/train_subset_embeddings_dataset.pkl', 'wb') as f:  # open a text file\n",
    "#     pickle.dump(train_subset_embeddings_dataset, f) # serialize the list\n",
    "#     f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url', 'embeddings'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset_embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def',\n",
       " 'load_config',\n",
       " '(',\n",
       " 'path',\n",
       " ')',\n",
       " ':',\n",
       " 'with',\n",
       " 'path',\n",
       " '.',\n",
       " 'open',\n",
       " '(',\n",
       " \"'rb'\",\n",
       " ')',\n",
       " 'as',\n",
       " 'fi',\n",
       " ':',\n",
       " 'file_bytes',\n",
       " '=',\n",
       " 'fi',\n",
       " '.',\n",
       " 'read',\n",
       " '(',\n",
       " ')',\n",
       " 'config',\n",
       " '=',\n",
       " 'yaml',\n",
       " '.',\n",
       " 'load',\n",
       " '(',\n",
       " 'file_bytes',\n",
       " '.',\n",
       " 'decode',\n",
       " '(',\n",
       " \"'utf-8'\",\n",
       " ')',\n",
       " ')',\n",
       " 'return',\n",
       " 'config']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_subset[0][\"func_code_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsed_DF = train_subset_embeddings_dataset.to_pandas() #train-subset-embeddings-dataset_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_code_tokens(lst):\n",
    "    result = string.punctuation \n",
    "    new_lst = [] \n",
    "    for character in lst:\n",
    "        if character in result:\n",
    "            continue\n",
    "        else:\n",
    "            new_lst.append(character)\n",
    "    return new_lst\n",
    "\n",
    "\n",
    "# # Creating inverted index based off this article: https://www.geeksforgeeks.org/inverted-index/\n",
    "# def make_documents(data, col_name):\n",
    "#     documents = data[col_name].dropna().apply(process_text).to_dict()\n",
    "#     return documents\n",
    "\n",
    "# def make_inverted_index(documents):\n",
    "#     word_array = np.array(list(documents.values()))\n",
    "#     all_words = []\n",
    "#     for words in word_array:\n",
    "#         all_words +=  words.split(\" \")\n",
    "# #     terms = dict(zip( range(len(set(all_words))),set(all_words)))\n",
    "# #     return terms\n",
    "#     all_words = set(all_words)\n",
    "#     inverted_index = {}\n",
    "    \n",
    "#     for word in all_words:\n",
    "#         if word != \"\":\n",
    "#             lst_docs = []\n",
    "#             for i, doc in documents.items():\n",
    "#                 if word in doc.split():\n",
    "#                     lst_docs.append(i)\n",
    "        \n",
    "#             inverted_index[word] = lst_docs\n",
    "#     return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "#Cleaned func_code_tokens and set to \"clean_code_tokens\"\n",
    "tsed_DF[\"clean_code_tokens\"] =  tsed_DF[\"func_code_tokens\"].apply(clean_code_tokens)\n",
    "\n",
    "# list(tsed_DF[\"clean_code_tokens\"].to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = tsed_DF[\"clean_code_tokens\"].to_dict()\n",
    "\n",
    "all_words = []\n",
    "for i in list(tsed_DF[\"clean_code_tokens\"].to_dict().values()):\n",
    "    all_words += i\n",
    "\n",
    "all_words = set(all_words)\n",
    "all_words\n",
    "\n",
    "inverted_index = {}\n",
    "\n",
    "for word in all_words:\n",
    "        if word != \"\":\n",
    "            lst_docs = []\n",
    "            for i, doc in documents.items():\n",
    "                if word in doc:\n",
    "                    lst_docs.append(i)\n",
    "        \n",
    "            inverted_index[word] = lst_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle inverted indices\n",
    "inverted_index\n",
    "\n",
    "\n",
    "# with open('./pickleObjects/inverted_index.pkl', 'wb') as f:  # open a text file\n",
    "#     pickle.dump(inverted_index, f) # serialize the list\n",
    "#     f.close()\n",
    "\n",
    "with open('./pickleObjects/inverted_index_1000.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(inverted_index, f) # serialize the list\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_DF(documents, inverted_index, total_vocab):\n",
    "    tf_idf = {}\n",
    "    df = pd.DataFrame()\n",
    "    for i, doc in documents.items():\n",
    "        term_lst = []\n",
    "        for term in total_vocab:\n",
    "            # doc_lst = doc.split()\n",
    "            tf = doc.count(term) / len(doc)\n",
    "\n",
    "            idf = np.log(len(documents) / len(inverted_index[term]))\n",
    "    #         if tf*idf > 0:\n",
    "    #             print(tf*idf)\n",
    "    #             print(term)\n",
    "            term_lst.append(tf*idf)\n",
    "            tf_idf[i, term] = tf*idf\n",
    "        df[i] = term_lst\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\halom\\AppData\\Local\\Temp\\ipykernel_20704\\1623084836.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[i] = term_lst\n"
     ]
    }
   ],
   "source": [
    "total_vocab = [x for x in inverted_index]\n",
    "\n",
    "tfidf_DF = make_tfidf_DF(documents, inverted_index, total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12330000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_vocab) * 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12326</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12327</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12328</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12330 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  990  991  992  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "12325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "12326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "12327  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "12328  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "12329  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "       993  994  995  996  997  998  999  \n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "12325  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12327  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12328  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "12329  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[12330 rows x 1000 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process query. Make it into a vector of tf-idfs\n",
    "def process_query(s, inverted_index, total_vocab, documents):\n",
    "    \n",
    "#     print(processed_s)\n",
    "    lst_words = s.split()\n",
    "#     print(lst_words)\n",
    "    q = np.zeros(len(total_vocab))\n",
    "#     print(len(q))\n",
    "    counter = Counter(lst_words)\n",
    "    for word in np.unique(lst_words):\n",
    "        if word in inverted_index:\n",
    "            tf = counter[word] / len(lst_words)\n",
    "            df = len(inverted_index[word])\n",
    "            idf = np.log(len(documents) / df)\n",
    "            q[total_vocab.index(word)] = tf*idf\n",
    "    \n",
    "    return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_query(\"flatten nested loop python\", inverted_index, total_vocab, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got from William Scott https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repository_name</th>\n",
       "      <th>func_path_in_repository</th>\n",
       "      <th>func_name</th>\n",
       "      <th>whole_func_string</th>\n",
       "      <th>language</th>\n",
       "      <th>func_code_string</th>\n",
       "      <th>func_code_tokens</th>\n",
       "      <th>func_documentation_string</th>\n",
       "      <th>func_documentation_tokens</th>\n",
       "      <th>split_name</th>\n",
       "      <th>func_code_url</th>\n",
       "      <th>embeddings</th>\n",
       "      <th>clean_code_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dillonhicks/rekt</td>\n",
       "      <td>rekt/utils.py</td>\n",
       "      <td>load_config</td>\n",
       "      <td>def load_config(path):\\n   \"\"\"\\n   Loads a yam...</td>\n",
       "      <td>python</td>\n",
       "      <td>def load_config(path):\\n   \"\"\"\\n   Loads a yam...</td>\n",
       "      <td>[def, load_config, (, path, ), :, with, path, ...</td>\n",
       "      <td>Loads a yaml configuration.\\n\\n   :param path:...</td>\n",
       "      <td>[Loads, a, yaml, configuration, .]</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/dillonhicks/rekt/blob/3848b...</td>\n",
       "      <td>[-0.057537895, -0.52463245, -0.03275, -0.05936...</td>\n",
       "      <td>[def, load_config, path, with, path, open, 'rb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google/closure-compiler</td>\n",
       "      <td>src/com/google/javascript/jscomp/DotFormatter....</td>\n",
       "      <td>DotFormatter.appendDot</td>\n",
       "      <td>static void appendDot(Node n, ControlFlowGraph...</td>\n",
       "      <td>java</td>\n",
       "      <td>static void appendDot(Node n, ControlFlowGraph...</td>\n",
       "      <td>[static, void, appendDot, (, Node, n, ,, Contr...</td>\n",
       "      <td>Converts an AST to dot representation and appe...</td>\n",
       "      <td>[Converts, an, AST, to, dot, representation, a...</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/google/closure-compiler/blo...</td>\n",
       "      <td>[-0.13995743, -0.6146675, -0.11137302, 0.05012...</td>\n",
       "      <td>[static, void, appendDot, Node, n, ControlFlow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenLiberty/open-liberty</td>\n",
       "      <td>dev/com.ibm.ws.messaging.common/src/com/ibm/ws...</td>\n",
       "      <td>ControlHighestGeneratedTickImpl.getTraceSummar...</td>\n",
       "      <td>public void getTraceSummaryLine(StringBuilder ...</td>\n",
       "      <td>java</td>\n",
       "      <td>public void getTraceSummaryLine(StringBuilder ...</td>\n",
       "      <td>[public, void, getTraceSummaryLine, (, StringB...</td>\n",
       "      <td>/*\\nGet summary trace line for this message\\n\\...</td>\n",
       "      <td>[/, *, Get, summary, trace, line, for, this, m...</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/OpenLiberty/open-liberty/bl...</td>\n",
       "      <td>[-0.17273049, -0.3561717, -0.11823353, 0.12623...</td>\n",
       "      <td>[public, void, getTraceSummaryLine, StringBuil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>keybase/client</td>\n",
       "      <td>go/kbfs/libdokan/dir.go</td>\n",
       "      <td>CanDeleteDirectory</td>\n",
       "      <td>func (d *Dir) CanDeleteDirectory(ctx context.C...</td>\n",
       "      <td>go</td>\n",
       "      <td>func (d *Dir) CanDeleteDirectory(ctx context.C...</td>\n",
       "      <td>[func, (, d, *, Dir, ), CanDeleteDirectory, (,...</td>\n",
       "      <td>// CanDeleteDirectory - return just nil\\n// TO...</td>\n",
       "      <td>[CanDeleteDirectory, -, return, just, nil, TOD...</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/keybase/client/blob/b352622...</td>\n",
       "      <td>[0.12481375, 0.041516956, -0.22884864, -0.0181...</td>\n",
       "      <td>[func, d, Dir, CanDeleteDirectory, ctx, contex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jblas-project/jblas</td>\n",
       "      <td>src/main/java/org/jblas/Solve.java</td>\n",
       "      <td>Solve.pinv</td>\n",
       "      <td>public static DoubleMatrix pinv(DoubleMatrix A...</td>\n",
       "      <td>java</td>\n",
       "      <td>public static DoubleMatrix pinv(DoubleMatrix A...</td>\n",
       "      <td>[public, static, DoubleMatrix, pinv, (, Double...</td>\n",
       "      <td>Computes the pseudo-inverse.\\n\\nNote, this fun...</td>\n",
       "      <td>[Computes, the, pseudo, -, inverse, .]</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/jblas-project/jblas/blob/28...</td>\n",
       "      <td>[0.101087354, -0.43265706, -0.16817348, -0.188...</td>\n",
       "      <td>[public, static, DoubleMatrix, pinv, DoubleMat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>brocade/pynos</td>\n",
       "      <td>pynos/versions/ver_6/ver_6_0_1/yang/brocade_nt...</td>\n",
       "      <td>brocade_ntp.show_ntp_output_node_active_server...</td>\n",
       "      <td>def show_ntp_output_node_active_server_rbridge...</td>\n",
       "      <td>python</td>\n",
       "      <td>def show_ntp_output_node_active_server_rbridge...</td>\n",
       "      <td>[def, show_ntp_output_node_active_server_rbrid...</td>\n",
       "      <td>Auto Generated Code</td>\n",
       "      <td>[Auto, Generated, Code]</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/brocade/pynos/blob/bd8a34e9...</td>\n",
       "      <td>[-0.02366774, -0.3845483, -0.25812605, 0.00860...</td>\n",
       "      <td>[def, show_ntp_output_node_active_server_rbrid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>sixty-north/cosmic-ray</td>\n",
       "      <td>src/cosmic_ray/cli.py</td>\n",
       "      <td>handle_exec</td>\n",
       "      <td>def handle_exec(args):\\n    \"\"\"usage: cosmic-r...</td>\n",
       "      <td>python</td>\n",
       "      <td>def handle_exec(args):\\n    \"\"\"usage: cosmic-r...</td>\n",
       "      <td>[def, handle_exec, (, args, ), :, session_file...</td>\n",
       "      <td>usage: cosmic-ray exec &lt;session-file&gt;\\n\\n    P...</td>\n",
       "      <td>[usage, :, cosmic, -, ray, exec, &lt;session, -, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/sixty-north/cosmic-ray/blob...</td>\n",
       "      <td>[-0.1944713, -0.41359708, -0.14166526, -0.3504...</td>\n",
       "      <td>[def, handle_exec, args, session_file, get_db_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>wonambi-python/wonambi</td>\n",
       "      <td>wonambi/attr/annotations.py</td>\n",
       "      <td>Annotations.set_stage_for_epoch</td>\n",
       "      <td>def set_stage_for_epoch(self, epoch_start, nam...</td>\n",
       "      <td>python</td>\n",
       "      <td>def set_stage_for_epoch(self, epoch_start, nam...</td>\n",
       "      <td>[def, set_stage_for_epoch, (, self, ,, epoch_s...</td>\n",
       "      <td>Change the stage for one specific epoch.\\n\\n  ...</td>\n",
       "      <td>[Change, the, stage, for, one, specific, epoch...</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/wonambi-python/wonambi/blob...</td>\n",
       "      <td>[-0.5645278, 0.013677543, -0.056194477, -0.112...</td>\n",
       "      <td>[def, set_stage_for_epoch, self, epoch_start, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>titon/toolkit</td>\n",
       "      <td>dist/toolkit.js</td>\n",
       "      <td></td>\n",
       "      <td>function(element, options) {\\n        element ...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>function(element, options) {\\n        element ...</td>\n",
       "      <td>[function, (, element, ,, options, ), {, eleme...</td>\n",
       "      <td>Initialize the pin.\\n\\n@param {jQuery} element...</td>\n",
       "      <td>[Initialize, the, pin, .]</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/titon/toolkit/blob/f0ed36d1...</td>\n",
       "      <td>[-0.092111215, -0.49511465, -0.1434023, -0.178...</td>\n",
       "      <td>[function, element, options, element, this, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>xhtml2pdf/xhtml2pdf</td>\n",
       "      <td>xhtml2pdf/w3c/cssParser.py</td>\n",
       "      <td>CSSParser._parseExpressionTerm</td>\n",
       "      <td>def _parseExpressionTerm(self, src):\\n        ...</td>\n",
       "      <td>python</td>\n",
       "      <td>def _parseExpressionTerm(self, src):\\n        ...</td>\n",
       "      <td>[def, _parseExpressionTerm, (, self, ,, src, )...</td>\n",
       "      <td>term\\n        : unary_operator?\\n            [...</td>\n",
       "      <td>[term, :, unary_operator?, [, NUMBER, S, *, |,...</td>\n",
       "      <td>train</td>\n",
       "      <td>https://github.com/xhtml2pdf/xhtml2pdf/blob/23...</td>\n",
       "      <td>[-0.24506398, -0.30638304, -0.1794964, -0.1979...</td>\n",
       "      <td>[def, _parseExpressionTerm, self, src, ctxsrc,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              repository_name  \\\n",
       "0            dillonhicks/rekt   \n",
       "1     google/closure-compiler   \n",
       "2    OpenLiberty/open-liberty   \n",
       "3              keybase/client   \n",
       "4         jblas-project/jblas   \n",
       "..                        ...   \n",
       "995             brocade/pynos   \n",
       "996    sixty-north/cosmic-ray   \n",
       "997    wonambi-python/wonambi   \n",
       "998             titon/toolkit   \n",
       "999       xhtml2pdf/xhtml2pdf   \n",
       "\n",
       "                               func_path_in_repository  \\\n",
       "0                                        rekt/utils.py   \n",
       "1    src/com/google/javascript/jscomp/DotFormatter....   \n",
       "2    dev/com.ibm.ws.messaging.common/src/com/ibm/ws...   \n",
       "3                              go/kbfs/libdokan/dir.go   \n",
       "4                   src/main/java/org/jblas/Solve.java   \n",
       "..                                                 ...   \n",
       "995  pynos/versions/ver_6/ver_6_0_1/yang/brocade_nt...   \n",
       "996                              src/cosmic_ray/cli.py   \n",
       "997                        wonambi/attr/annotations.py   \n",
       "998                                    dist/toolkit.js   \n",
       "999                         xhtml2pdf/w3c/cssParser.py   \n",
       "\n",
       "                                             func_name  \\\n",
       "0                                          load_config   \n",
       "1                               DotFormatter.appendDot   \n",
       "2    ControlHighestGeneratedTickImpl.getTraceSummar...   \n",
       "3                                   CanDeleteDirectory   \n",
       "4                                           Solve.pinv   \n",
       "..                                                 ...   \n",
       "995  brocade_ntp.show_ntp_output_node_active_server...   \n",
       "996                                        handle_exec   \n",
       "997                    Annotations.set_stage_for_epoch   \n",
       "998                                                      \n",
       "999                     CSSParser._parseExpressionTerm   \n",
       "\n",
       "                                     whole_func_string    language  \\\n",
       "0    def load_config(path):\\n   \"\"\"\\n   Loads a yam...      python   \n",
       "1    static void appendDot(Node n, ControlFlowGraph...        java   \n",
       "2    public void getTraceSummaryLine(StringBuilder ...        java   \n",
       "3    func (d *Dir) CanDeleteDirectory(ctx context.C...          go   \n",
       "4    public static DoubleMatrix pinv(DoubleMatrix A...        java   \n",
       "..                                                 ...         ...   \n",
       "995  def show_ntp_output_node_active_server_rbridge...      python   \n",
       "996  def handle_exec(args):\\n    \"\"\"usage: cosmic-r...      python   \n",
       "997  def set_stage_for_epoch(self, epoch_start, nam...      python   \n",
       "998  function(element, options) {\\n        element ...  javascript   \n",
       "999  def _parseExpressionTerm(self, src):\\n        ...      python   \n",
       "\n",
       "                                      func_code_string  \\\n",
       "0    def load_config(path):\\n   \"\"\"\\n   Loads a yam...   \n",
       "1    static void appendDot(Node n, ControlFlowGraph...   \n",
       "2    public void getTraceSummaryLine(StringBuilder ...   \n",
       "3    func (d *Dir) CanDeleteDirectory(ctx context.C...   \n",
       "4    public static DoubleMatrix pinv(DoubleMatrix A...   \n",
       "..                                                 ...   \n",
       "995  def show_ntp_output_node_active_server_rbridge...   \n",
       "996  def handle_exec(args):\\n    \"\"\"usage: cosmic-r...   \n",
       "997  def set_stage_for_epoch(self, epoch_start, nam...   \n",
       "998  function(element, options) {\\n        element ...   \n",
       "999  def _parseExpressionTerm(self, src):\\n        ...   \n",
       "\n",
       "                                      func_code_tokens  \\\n",
       "0    [def, load_config, (, path, ), :, with, path, ...   \n",
       "1    [static, void, appendDot, (, Node, n, ,, Contr...   \n",
       "2    [public, void, getTraceSummaryLine, (, StringB...   \n",
       "3    [func, (, d, *, Dir, ), CanDeleteDirectory, (,...   \n",
       "4    [public, static, DoubleMatrix, pinv, (, Double...   \n",
       "..                                                 ...   \n",
       "995  [def, show_ntp_output_node_active_server_rbrid...   \n",
       "996  [def, handle_exec, (, args, ), :, session_file...   \n",
       "997  [def, set_stage_for_epoch, (, self, ,, epoch_s...   \n",
       "998  [function, (, element, ,, options, ), {, eleme...   \n",
       "999  [def, _parseExpressionTerm, (, self, ,, src, )...   \n",
       "\n",
       "                             func_documentation_string  \\\n",
       "0    Loads a yaml configuration.\\n\\n   :param path:...   \n",
       "1    Converts an AST to dot representation and appe...   \n",
       "2    /*\\nGet summary trace line for this message\\n\\...   \n",
       "3    // CanDeleteDirectory - return just nil\\n// TO...   \n",
       "4    Computes the pseudo-inverse.\\n\\nNote, this fun...   \n",
       "..                                                 ...   \n",
       "995                                Auto Generated Code   \n",
       "996  usage: cosmic-ray exec <session-file>\\n\\n    P...   \n",
       "997  Change the stage for one specific epoch.\\n\\n  ...   \n",
       "998  Initialize the pin.\\n\\n@param {jQuery} element...   \n",
       "999  term\\n        : unary_operator?\\n            [...   \n",
       "\n",
       "                             func_documentation_tokens split_name  \\\n",
       "0                   [Loads, a, yaml, configuration, .]      train   \n",
       "1    [Converts, an, AST, to, dot, representation, a...      train   \n",
       "2    [/, *, Get, summary, trace, line, for, this, m...      train   \n",
       "3    [CanDeleteDirectory, -, return, just, nil, TOD...      train   \n",
       "4               [Computes, the, pseudo, -, inverse, .]      train   \n",
       "..                                                 ...        ...   \n",
       "995                            [Auto, Generated, Code]      train   \n",
       "996  [usage, :, cosmic, -, ray, exec, <session, -, ...      train   \n",
       "997  [Change, the, stage, for, one, specific, epoch...      train   \n",
       "998                          [Initialize, the, pin, .]      train   \n",
       "999  [term, :, unary_operator?, [, NUMBER, S, *, |,...      train   \n",
       "\n",
       "                                         func_code_url  \\\n",
       "0    https://github.com/dillonhicks/rekt/blob/3848b...   \n",
       "1    https://github.com/google/closure-compiler/blo...   \n",
       "2    https://github.com/OpenLiberty/open-liberty/bl...   \n",
       "3    https://github.com/keybase/client/blob/b352622...   \n",
       "4    https://github.com/jblas-project/jblas/blob/28...   \n",
       "..                                                 ...   \n",
       "995  https://github.com/brocade/pynos/blob/bd8a34e9...   \n",
       "996  https://github.com/sixty-north/cosmic-ray/blob...   \n",
       "997  https://github.com/wonambi-python/wonambi/blob...   \n",
       "998  https://github.com/titon/toolkit/blob/f0ed36d1...   \n",
       "999  https://github.com/xhtml2pdf/xhtml2pdf/blob/23...   \n",
       "\n",
       "                                            embeddings  \\\n",
       "0    [-0.057537895, -0.52463245, -0.03275, -0.05936...   \n",
       "1    [-0.13995743, -0.6146675, -0.11137302, 0.05012...   \n",
       "2    [-0.17273049, -0.3561717, -0.11823353, 0.12623...   \n",
       "3    [0.12481375, 0.041516956, -0.22884864, -0.0181...   \n",
       "4    [0.101087354, -0.43265706, -0.16817348, -0.188...   \n",
       "..                                                 ...   \n",
       "995  [-0.02366774, -0.3845483, -0.25812605, 0.00860...   \n",
       "996  [-0.1944713, -0.41359708, -0.14166526, -0.3504...   \n",
       "997  [-0.5645278, 0.013677543, -0.056194477, -0.112...   \n",
       "998  [-0.092111215, -0.49511465, -0.1434023, -0.178...   \n",
       "999  [-0.24506398, -0.30638304, -0.1794964, -0.1979...   \n",
       "\n",
       "                                     clean_code_tokens  \n",
       "0    [def, load_config, path, with, path, open, 'rb...  \n",
       "1    [static, void, appendDot, Node, n, ControlFlow...  \n",
       "2    [public, void, getTraceSummaryLine, StringBuil...  \n",
       "3    [func, d, Dir, CanDeleteDirectory, ctx, contex...  \n",
       "4    [public, static, DoubleMatrix, pinv, DoubleMat...  \n",
       "..                                                 ...  \n",
       "995  [def, show_ntp_output_node_active_server_rbrid...  \n",
       "996  [def, handle_exec, args, session_file, get_db_...  \n",
       "997  [def, set_stage_for_epoch, self, epoch_start, ...  \n",
       "998  [function, element, options, element, this, se...  \n",
       "999  [def, _parseExpressionTerm, self, src, ctxsrc,...  \n",
       "\n",
       "[1000 rows x 13 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsed_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_matches(query, k, alpha = 0.5):\n",
    "    q_vector = process_query(query, inverted_index, total_vocab, documents)\n",
    "    q_embedding_vector = get_embeddings([query]).cpu().detach().numpy()[0]\n",
    "    \n",
    "  \n",
    "#     print(len(q_vector_space))\n",
    "#     print(len(q_vector_title))\n",
    "    \n",
    "    cosine_lst = [[x,0] for x in range(len(tfidf_DF.columns.to_list()))]\n",
    "\n",
    "#     print(len(cosine_lst_title))\n",
    "#     print(len(cosine_lst_space))\n",
    "    \n",
    "    for x in tfidf_DF.columns.to_list():\n",
    "        col = tfidf_DF[x].to_numpy()\n",
    "        # Tensor.cpu()\n",
    "        embedding = tsed_DF.iloc[x][\"embeddings\"]\n",
    "\n",
    "        cosine_lst[x] = [x, (alpha) * cosine_sim(q_vector, col) + (1 - alpha) * cosine_sim(q_embedding_vector, embedding)]\n",
    "    \n",
    "    \n",
    "    cosine_lst.sort(reverse = True, key = lambda x: x[1])\n",
    "    return cosine_lst[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split.apply\n",
      "/*\n",
      "split(input, delimiter = ' ')\n",
      "\n",
      "Split a string on a matching pattern\n",
      "\n",
      "E.g. {{ \"a~b\" | split:'~' | first }} #=> 'a'\n",
      "SCORE: 0.5327710579078077\n",
      "----------------------------------------------------------------------------------------------------\n",
      "explicit_line_join\n",
      "r\"\"\"Avoid explicit line join between brackets.\n",
      "\n",
      "    The preferred way of wrapping long lines is by using Python's implied line\n",
      "    continuation inside parentheses, brackets and braces.  Long lines can be\n",
      "    broken over multiple lines by wrapping expressions in parentheses.  These\n",
      "    should be used in preference to using a backslash for line continuation.\n",
      "\n",
      "    E502: aaa = [123, \\\\n       123]\n",
      "    E502: aaa = (\"bbb \" \\\\n       \"ccc\")\n",
      "\n",
      "    Okay: aaa = [123,\\n       123]\n",
      "    Okay: aaa = (\"bbb \"\\n       \"ccc\")\n",
      "    Okay: aaa = \"bbb \" \\\\n    \"ccc\"\n",
      "    Okay: aaa = 123  # \\\\\n",
      "SCORE: 0.4863744735717774\n",
      "----------------------------------------------------------------------------------------------------\n",
      "StringCharacterIterator.next\n",
      "Implements CharacterIterator.next() for String.\n",
      "@see CharacterIterator#next\n",
      "@deprecated ICU 2.4. Use java.text.StringCharacterIterator instead.\n",
      "SCORE: 0.3577323913574219\n",
      "----------------------------------------------------------------------------------------------------\n",
      "DSVParser._CreateDictReader\n",
      "Returns a reader that processes each row and yields dictionaries.\n",
      "\n",
      "    csv.DictReader does this job well for single-character delimiters; parsers\n",
      "    that need multi-character delimiters need to override this method.\n",
      "\n",
      "    Args:\n",
      "      line_reader (iter): yields lines from a file-like object.\n",
      "\n",
      "    Returns:\n",
      "      iter: a reader of dictionaries, as returned by csv.DictReader().\n",
      "SCORE: 0.34507567882537843\n",
      "----------------------------------------------------------------------------------------------------\n",
      "StreamList.redraw_current_line\n",
      "Redraw the highlighted line\n",
      "SCORE: 0.3425897598266602\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Segments.add_contact\n",
      "Add a contact to the segment\n",
      "\n",
      "        :param segment_id: int Segment ID\n",
      "        :param contact_id: int Contact ID\n",
      "        :return: dict|str\n",
      "SCORE: 0.3386098384857178\n",
      "----------------------------------------------------------------------------------------------------\n",
      "WithTimeout\n",
      "// WithTimeout returns WithDeadline(time.Now().Add(timeout)).\n",
      "SCORE: 0.33454008102416993\n",
      "----------------------------------------------------------------------------------------------------\n",
      "push\n",
      "Input:  {\n",
      "              (path)  - repo UOA (where to create entry)\n",
      "              (type)  - type\n",
      "              (url)   - URL\n",
      "\n",
      "                or\n",
      "\n",
      "              (data_uoa)  - repo UOA\n",
      "\n",
      "              (clone) - if 'yes', clone repo instead of update\n",
      "            }\n",
      "\n",
      "    Output: {\n",
      "              return       - return code =  0, if successful\n",
      "                                         >  0, if error\n",
      "              (error)      - error text if return > 0\n",
      "            }\n",
      "SCORE: 0.33399004936218263\n",
      "----------------------------------------------------------------------------------------------------\n",
      "LTrim\n",
      "// LTrim trim an existing list so that it will contain only the specified range of elements specified.\n",
      "// Both start and stop are zero-based indexes, where 0 is the first element of the list (the head),\n",
      "// 1 the next element and so on.\n",
      "SCORE: 0.3318352299584736\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decode\n",
      "// Decode\n",
      "SCORE: 0.3295579176674457\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for lst in find_best_matches(\"How to split string by newline PYTHON\", 10, 0.2):\n",
    "    print(tsed_DF.iloc[lst[0]][\"func_name\"])\n",
    "    print(tsed_DF.iloc[lst[0]][\"func_documentation_string\"])\n",
    "    print(f\"SCORE: {lst[1]}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
