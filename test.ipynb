{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import datasets\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "# import string library function  \n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"code_search_net\", \"all\")\n",
    "\n",
    "dataset_dict = datasets.load_from_disk(\"./Dataset/CodeSearchCorpus/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.backends.cudnn.enabled)\n",
    "print(torch.cuda.is_available()) #We have GPU on deck and ready\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset_dict[\"train\"]))\n",
    "print(len(dataset_dict[\"validation\"]))\n",
    "print(len(dataset_dict[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_dict[\"train\"]\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset_dict[\"test\"]\n",
    "test_dataset\n",
    "\n",
    "# Yeah, 1.8M is too much. For week 5 at least, we've decided to train on a random sample of 10k from the training, 1k validation and 1k test\n",
    "\n",
    "# Column for semantic search: func_documentation_string\n",
    "# Column for tfidf: func_code_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 20000\n",
    "filepath_pkl_obj = \"./PickleObjects/\"\n",
    "inverted_index_name = f\"inverted_index_{num_rows}.pkl\"\n",
    "tsed_name = f\"train_subset_embeddings_dataset_{num_rows}.pkl\"\n",
    "\n",
    "print(inverted_index_name, tsed_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "train_subset_indices = np.random.choice(len(train_dataset), num_rows, replace = False)\n",
    "train_dataset_subset = train_dataset.select(train_subset_indices)\n",
    "\n",
    "len(train_dataset_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device\n",
    "\n",
    "#Following code from: https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Hugging Face Tutorials\n",
    "def cls_pooling(model_output):\n",
    "    return model_output.last_hidden_state[:, 0]\n",
    "\n",
    "def get_embeddings(text_list):\n",
    "    encoded_input = tokenizer(\n",
    "        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n",
    "    model_output = model(**encoded_input)\n",
    "    return cls_pooling(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit: https://huggingface.co/docs/datasets/use_with_pytorch\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained embeddings for semantic search portion\n",
    "f'{filepath_pkl_obj}{tsed_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train embeddings\n",
    "\n",
    "#REMEMBER TO KEEP THE FILENAMES THE SAME 0_0\n",
    "try:\n",
    "    with open(f'{filepath_pkl_obj}{tsed_name}', 'rb') as f:  # open a text file\n",
    "        train_subset_embeddings_dataset = pickle.load(f) # serialize the list\n",
    "        f.close()\n",
    "except:\n",
    "    train_subset_embeddings_dataset = train_dataset_subset.map(\n",
    "        lambda x: {\"embeddings\": get_embeddings(x[\"func_documentation_string\"]).detach().cpu().numpy()[0]}\n",
    "    )\n",
    "\n",
    "    with open(f'{filepath_pkl_obj}{tsed_name}', 'wb') as f:  # open a text file\n",
    "        pickle.dump(train_subset_embeddings_dataset, f) # serialize the list\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'{filepath_pkl_obj}{tsed_name}', 'wb') as f:  # open a text file\n",
    "#         pickle.dump(train_subset_embeddings_dataset, f) # serialize the list\n",
    "#         f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('./pickleObjects/train_subset_embeddings_dataset.pkl', 'wb') as f:  # open a text file\n",
    "#     pickle.dump(train_subset_embeddings_dataset, f) # serialize the list\n",
    "#     f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset_embeddings_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionary for tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_subset[0][\"func_code_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsed_DF = train_subset_embeddings_dataset.to_pandas() #train-subset-embeddings-dataset_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_code_tokens(lst):\n",
    "    result = string.punctuation \n",
    "    new_lst = [] \n",
    "    for character in lst:\n",
    "        if character in result:\n",
    "            continue\n",
    "        else:\n",
    "            new_lst.append(character)\n",
    "    return new_lst\n",
    "\n",
    "\n",
    "# # Creating inverted index based off this article: https://www.geeksforgeeks.org/inverted-index/\n",
    "# def make_documents(data, col_name):\n",
    "#     documents = data[col_name].dropna().apply(process_text).to_dict()\n",
    "#     return documents\n",
    "\n",
    "# def make_inverted_index(documents):\n",
    "#     word_array = np.array(list(documents.values()))\n",
    "#     all_words = []\n",
    "#     for words in word_array:\n",
    "#         all_words +=  words.split(\" \")\n",
    "# #     terms = dict(zip( range(len(set(all_words))),set(all_words)))\n",
    "# #     return terms\n",
    "#     all_words = set(all_words)\n",
    "#     inverted_index = {}\n",
    "    \n",
    "#     for word in all_words:\n",
    "#         if word != \"\":\n",
    "#             lst_docs = []\n",
    "#             for i, doc in documents.items():\n",
    "#                 if word in doc.split():\n",
    "#                     lst_docs.append(i)\n",
    "        \n",
    "#             inverted_index[word] = lst_docs\n",
    "#     return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "#Cleaned func_code_tokens and set to \"clean_code_tokens\"\n",
    "tsed_DF[\"clean_code_tokens\"] =  tsed_DF[\"func_code_tokens\"].apply(clean_code_tokens)\n",
    "\n",
    "# list(tsed_DF[\"clean_code_tokens\"].to_dict().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{filepath_pkl_obj}{inverted_index_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much of this code was based off of William Scott's implementation of TF-IDF: https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = tsed_DF[\"clean_code_tokens\"].to_dict()\n",
    "\n",
    "all_words = []\n",
    "for i in list(tsed_DF[\"clean_code_tokens\"].to_dict().values()):\n",
    "    all_words += i\n",
    "\n",
    "all_words = list(set(all_words)) #Get rid of all repeats\n",
    "all_words\n",
    "\n",
    "try:\n",
    "     with open(f'{filepath_pkl_obj}{inverted_index_name}.pkl', 'rb') as f:\n",
    "        inverted_index = pickle.load(f) # deserialize using load()\n",
    "        f.close()\n",
    "except:\n",
    "    inverted_index = {}\n",
    "\n",
    "    for word in all_words:\n",
    "            if word != \"\":\n",
    "                lst_docs = []\n",
    "                for i, doc in documents.items():\n",
    "                    if word in doc:\n",
    "                        lst_docs.append(i)\n",
    "            \n",
    "                inverted_index[word] = lst_docs\n",
    "    \n",
    "    #Pickle afterwards\n",
    "    with open(f'{filepath_pkl_obj}{inverted_index_name}.pkl', 'wb') as f:  # open a text file\n",
    "        pickle.dump(inverted_index, f) # serialize the list\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words) == len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle inverted indices\n",
    "len(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = inverted_index[word]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if type(c) == list:\n",
    "        return len(c)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "#number of rows sampled\n",
    "tf_idf = {}\n",
    "for i in range(num_rows):\n",
    "    # print(i)\n",
    "    tokens = tsed_DF[\"clean_code_tokens\"].iloc[i]\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        tf = counter[token] / words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((num_rows + 1) / (df + 1))\n",
    "\n",
    "        tf_idf[i, token] = tf * idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_tfidf_DF(documents, inverted_index, total_vocab):\n",
    "#     tf_idf = {}\n",
    "#     df = pd.DataFrame()\n",
    "#     for i, doc in documents.items():\n",
    "#         term_lst = []\n",
    "#         for term in total_vocab:\n",
    "#             # doc_lst = doc.split()\n",
    "#             tf = doc.count(term) / len(doc)\n",
    "\n",
    "#             idf = np.log(len(documents) / len(inverted_index[term]))\n",
    "#     #         if tf*idf > 0:\n",
    "#     #             print(tf*idf)\n",
    "#     #             print(term)\n",
    "#             term_lst.append(tf*idf)\n",
    "#             tf_idf[i, term] = tf*idf\n",
    "#         df[i] = term_lst\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_dict = dict(zip(all_words, range(len(all_words))))\n",
    "\n",
    "\n",
    "tf_idf_array = np.zeros((num_rows, len(all_words)))\n",
    "\n",
    "for i in tf_idf:\n",
    "    try:\n",
    "        ind = all_words_dict[i[1]]\n",
    "        tf_idf_array[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(s):\n",
    "    # This is where we'd do more processing of the query\n",
    "    tokens = s.split()\n",
    "\n",
    "    q_vector = np.zeros((len(all_words)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((num_rows+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = all_words_dict[token]\n",
    "            q_vector[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return q_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_vector(\"pandas how to select first 10 rows\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_array[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Process query. Make it into a vector of tf-idfs\n",
    "# def process_query(s, inverted_index, total_vocab, documents):\n",
    "    \n",
    "# #     print(processed_s)\n",
    "#     lst_words = s.split()\n",
    "# #     print(lst_words)\n",
    "#     q = np.zeros(len(total_vocab))\n",
    "# #     print(len(q))\n",
    "#     counter = Counter(lst_words)\n",
    "#     for word in np.unique(lst_words):\n",
    "#         if word in inverted_index:\n",
    "#             tf = counter[word] / len(lst_words)\n",
    "#             df = len(inverted_index[word])\n",
    "#             idf = np.log(len(documents) / df)\n",
    "#             q[total_vocab.index(word)] = tf*idf\n",
    "    \n",
    "#     return q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_query(\"flatten nested loop python\", inverted_index, total_vocab, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got from William Scott https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsed_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_matches(query, k, alpha = 0.5):\n",
    "    q_vector = gen_vector(query)\n",
    "    q_embedding_vector = get_embeddings([query]).cpu().detach().numpy()[0]\n",
    "    \n",
    "    # print(q_vector)\n",
    "    # print(q_vector.shape)\n",
    "#     print(len(q_vector_space))\n",
    "#     print(len(q_vector_title))\n",
    "    \n",
    "    cosine_lst = [[x,0] for x in range(num_rows)]\n",
    "\n",
    "#     print(len(cosine_lst_title))\n",
    "#     print(len(cosine_lst_space))\n",
    "    \n",
    "    for i, x in enumerate(tf_idf_array):\n",
    "        # col = tfidf_DF[x].to_numpy()\n",
    "        # Tensor.cpu()\n",
    "        embedding = tsed_DF.iloc[i][\"embeddings\"]\n",
    "\n",
    "        cosine_lst[i] = [i, (alpha) * cosine_sim(q_vector, x) + (1 - alpha) * cosine_sim(q_embedding_vector, embedding)]\n",
    "    \n",
    "    \n",
    "    cosine_lst.sort(reverse = True, key = lambda x: x[1])\n",
    "    return cosine_lst[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbm_result = find_best_matches(\"How to split string by newline PYTHON\", 10, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsed_DF.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_lst = []\n",
    "# func_code_url_lst = []\n",
    "# for lst in fbm_result:\n",
    "#     # print(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "#     # print(tsed_DF.iloc[lst[0]][\"func_name\"])\n",
    "#     # print(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "#     # print(f\"SCORE: {lst[1]}\")\n",
    "#     # print(\"-\" * 100)\n",
    "\n",
    "#     lang_lst.append(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "#     func_code_url_lst.append(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_lst\n",
    "# func_code_url_lst\n",
    "# pd.DataFrame({'language' : lang_lst, 'url': func_code_url_lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results(query_filepath, results_per_query = 100):\n",
    "    queries = pd.read_csv(query_filepath)\n",
    "    # display(queries)\n",
    "    q_lst = queries[\"query\"].to_list()\n",
    "    # print(q_lst)\n",
    "\n",
    "    lang_lst = []\n",
    "    func_code_url_lst = []\n",
    "    query_lst = []\n",
    "\n",
    "    for i, query in enumerate(q_lst):\n",
    "        print(i)\n",
    "        fbm_lst = find_best_matches(query, results_per_query, 0.2)\n",
    "        query_lst += [query for j in range(len(fbm_lst))]\n",
    "        \n",
    "        for lst in fbm_lst:\n",
    "            # print(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "            # print(tsed_DF.iloc[lst[0]][\"func_name\"])\n",
    "            # print(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "            # print(f\"SCORE: {lst[1]}\")\n",
    "            # print(\"-\" * 100)\n",
    "\n",
    "            lang_lst.append(tsed_DF.iloc[lst[0]][\"language\"])\n",
    "            func_code_url_lst.append(tsed_DF.iloc[lst[0]][\"func_code_url\"])\n",
    "        \n",
    "        # break\n",
    "\n",
    "    # print(lang_lst)\n",
    "    # print(func_code_url_lst)\n",
    "    # print(query_lst)\n",
    "    return lang_lst, func_code_url_lst, query_lst\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_lst, func_code_url_lst, query_lst = create_results(\"./Dataset/Testing/queries.csv\", results_per_query=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.DataFrame({'language' : lang_lst, 'url': func_code_url_lst, \"query\" : query_lst})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"./csv_output/baseline_20k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSC180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
